{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Dataset Visualisation\n",
    "The three generated datasets are plotted below. These datasets represent three common boundary situations that we would encounter in practice: linearly separable boundary in space, higher-order separable boundary, and linearly non-separable boundary. I chose these datasets because they are easy to visualise so that we can know how the algorithm works geometrically.\n",
    "\n",
    "![datasets.png](datasets.png)\n",
    "\n",
    "The CIFAR10 and CIFAR100 datasets are visualised by projecting the extracted features to two dimensional space with t-SNE algorithm.\n",
    "\n",
    "\n",
    "\n",
    "<p float=\"left\">\n",
    "    <img src=\"cifar10-tsne.png\"  width=\"30%\"/><img src=\"cifar100-tsne.png\"  width=\"30%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "Different colour represents different classes. It is clear that the shape of the projected features is similar with the blobs dataset. This actually indicates that the extracted features with pre-trained network are good for logistic regression algorithm. Therefore, the selection preference on the blob dataset should be able to reflect the behaviour on CIFAR10 and CIFAR100 only if the selection algorithm is based on the geometric location of the samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment  1: Analysis of the algorithms\n",
    "This experiment is designed to analyse the intrinsic behaviour of the algorithms individually. \n",
    "## 1.1 POP\n",
    "The POP algorithm assumes that keeping boundary points only can already ensure simple machine learning algorithms like knn and logistic regression to recover the desired decision boundary. Many papers have proved this hypothesis with critical experiments. However, for deep learning, the performance is still unknown. If we consider the network as extracting features first and classify the features with a logistic regression algorithm, then the problem is whether the network can extract high-quality features or not with POP selected samples. Please note that the POP is not expected to get the same extracted features because of the randomness of deep learning.\n",
    "\n",
    "For the generated blob dataset, the weakness countplot is shown below. Recall that the weakness is defined as the number of times that a sample is not a boundary sample by projecting the features to each axis. Therefore, only samples with weakness equal to 2 are purely inner samples. The right plot shows the selected samples in green. It indicates that if the classifier can separate the selected points, then it should be able to separate all the other samples as well. Actually, the test accuracy is\n",
    "99.5% if we use early stop method by selecting the model that can achieve the highest validation accuracy.\n",
    "\n",
    "<p float=\"left\">\n",
    "    <img src=\"pop-blob.png\"  width=\"45%\"/> <img src=\"pop-blob-368.png\"  width=\"45%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "However, for higher-order feature dimensions, there are less purely inner samples as defined by POP. The countplots of CIFAR10 (left) and CIFAR100 (right) are shown below.\n",
    "\n",
    "<p float=\"left\">\n",
    "    <img src=\"pop-cifar10.png\"  width=\"45%\"/> <img src=\"pop-cifar100.png\"  width=\"45%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "For CIFAR10 and CIFAR100, there are no purely inner samples (weakness == 128 since we use 128-dimensional features). Thus POP is less efficient for datasets with more classes and features.\n",
    "\n",
    "Also, the circles(left) and the moons (right)  POP weakness countplots are shown below. It is clear that the reduction rate of POP is highly contained by the geometric location of the samples. If the samples are not separatable after projecting them into a single axis, then POP is not a suitable algorithm. \n",
    "\n",
    "<p float=\"left\">\n",
    "    <img src=\"pop-circles.png\"  width=\"45%\"/><img src=\"pop-moons.png\"  width=\"45%\"/> \n",
    "</p>\n",
    "\n",
    "\n",
    "\n",
    "I varied the weakness threshold for CIFAR10 and CIFAR100 to see what will happen if I decide to remove not purely inner samples. The classification accuracy for CIFAR10 is shown in the table below. For CIFAR10, the results are still acceptable. However, for CIFAR100, it is meaningless to use POP because with weakness == 0 (pure boundary samples), there are already more than 85% samples selected and the relative accuracy is close to 1.\n",
    "\n",
    "<img src=\"pop-his.png\"  width=\"50%\"/>\n",
    "\n",
    "There are two possible ways to improve the POP algorithm performance. The first way is to remove the BatchNormalisation layer of the pre-trained feature extraction network so that the difference between feature values are higher and it would be easier to avoid samples with closer feature values but different class labels. The other way is to lower the numpy.isclose() tolerance so that samples with similar features will be considered as non-boundary samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 EGDIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EGDIS is designed to select both boundary samples and the densest samples. However, samples at the boundary are harder to classify compared with inner samples. Therefore, the test accuracy may be slightly lower than other algorithms if we train the network with the same amount of samples. The benefit is that these boundary samples are the key to achieve higher accuracy and lower loss. Researchers can focus on designing new networks or training methods to classify them right. To prove the hypothsis, I plotted the density of classification scores of the boundary samples below. The left side is CIFAR10 and the right side is CIFAR100. I didn't use the generated datasets to show this because these datasets are too simple and most of the scores are above 0.9%. Also, for blobs, there is only 1 boundary sample selected. For moons, there are 3 boundary samples selected. For circles, there are 35 boundary samples selected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"egdis-boundary-scores-cifar10.png\" width=\"50%\"/><img src=\"egdis-boundary-scores-cifar100.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CIFAR10, there are 3069 boundary samples. For CIFAR100, there are 12258 boundary samples. The reason why there are so many low-score samples is that the extracted features from NasNetLarge can only achieve a test accuracy of 71.60% (for CIFAR10, this value is 95.54,  which is close to the best accuracy that we can achieve). If we fine-tune the network with CIFAR100 in advance, the scores should be much higher and the boundary points should be much less. However, this is acceptable and indicates that it is harder to extract good features for these samples so that we can focus on these samples to improve the network performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualisation of the selected samples are shown below. For CIFAR10, the color is yellow and for CIFAR100, the color is pink. It is much harder to interpret the CIFAR100 plot because there are too many blobs but for CIFAR10, the selected points are located at the boundary between two  clusters and the inner region of the clusters.\n",
    "\n",
    "<img src=\"egdis-cifar10.png\" width=\"45%\"/> <img src=\"egdis-cifar100.png\" width=\"45%\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated datasets are shown below.  \n",
    "<img src=\"egdis-blobs.png\" width=\"32%\"/> <img src=\"egdis-moons.png\" width=\"32%\"/> <img src=\"egdis-circles.png\" width=\"32%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that most of the selected samples are located at the inner section of the clusters. Here EGDIS selected 136, 177, 202 samples respectively and achieved a test score of 0.993, 1.000, and 0.785. For circle dataset, the performance is not good. This is because to recover the decision boundary, we need samples all over the circle but the dense region is not distributed across the circle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CIFAR10, the EGDIS algorithm achieved a relative accuracy of 86.58% with 16.27% training samples. The results for CIFAR100 are still running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 CL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea of CL is to select samples with high classification scores. However, it is not designed to select subset samples, but to speed up the training speed by train with easy samples first. In this project, I modified the algorithm to choose the samples by weighting the samples with the classification scores. The advantage to use CL as data selection algorithm is that it is highly related with the behaviour of neural network. For this reason, CL should performam better than other algorithms.\n",
    "\n",
    "For generated datasets, the CL algorithm tends to be random selection because the scores for more than 95% samples are close to 1. Therefore, the selected samples are uniformly located at the clusters and the shape of the boundary is recovered. By selecting 10% of the samples (120 samples), the accuracy is higher than 95% for all the three datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CIFAR10, the selected samples are shown below. By selecting 10%, 30% and 50% of the samples according to the classification scores, we  can see that these samples are all far away from the boundary between two clusters. Therefore, a fair gauss is that although the convergence speed could be faster, the final accuracy may be lower because the boundary samples are not included. This situation is more serious if we choose only a small portion of the samples. However, this is not true when comparing CL with EGDIS by selecting 16.27% portion of samples. EGDIS achieved an accuracy of 82.44% but CL achieved 87.54%. A possible reason is that the scores are calculated from the pre-trained NasNet, which may not reflect the true difficulty of the samples.\n",
    "\n",
    "<img src=\"cl-cifar10_0.1.png\" width=\"30%\"> <img src=\"cl-cifar10_0.3.png\" width=\"30%\"> <img src=\"cl-cifar10_0.5.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that these analysis is not the final conclusion. The cifar100 experiments are still running and I plan to run more tests by selecting the same amount of samples with different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main drawback of CL is that it only selects easy samples so the accuracy is limited if the dataset is hard to classify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msc",
   "language": "python",
   "name": "msc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
